{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ed46ba",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url\n",
    "= https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details: A)\n",
    "Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08afa3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, WebDriverException\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d62625b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row format may be unexpected; skipping row.\n",
      "                                               Rank  \\\n",
      "0                             \"Baby Shark Dance\"[7]   \n",
      "1                                   \"Despacito\"[10]   \n",
      "2                        \"Johny Johny Yes Papa\"[18]   \n",
      "3                                   \"Bath Song\"[19]   \n",
      "4                           \"Wheels on the Bus\"[20]   \n",
      "5                               \"See You Again\"[21]   \n",
      "6                                \"Shape of You\"[26]   \n",
      "7                 \"Phonics Song with Two Words\"[29]   \n",
      "8                                 \"Uptown Funk\"[30]   \n",
      "9                               \"Gangnam Style\"[31]   \n",
      "10  \"Learning Colors – Colorful Eggs on a Farm\"[36]   \n",
      "11                                     \"Axel F\"[37]   \n",
      "12                             \"Dame Tu Cosita\"[38]   \n",
      "13   \"Masha and the Bear – Recipe for Disaster\"[39]   \n",
      "14                        \"Baa Baa Black Sheep\"[40]   \n",
      "15                             \"Lakdi Ki Kathi\"[41]   \n",
      "16                                      \"Sugar\"[42]   \n",
      "17                             \"Counting Stars\"[43]   \n",
      "18                                       \"Roar\"[44]   \n",
      "19           \"Waka Waka (This Time for Africa)\"[45]   \n",
      "20                      \"Shree Hanuman Chalisa\"[46]   \n",
      "21          \"Humpty the train on a fruits ride\"[47]   \n",
      "22                                      \"Sorry\"[48]   \n",
      "23                          \"Thinking Out Loud\"[49]   \n",
      "24                                    \"Perfect\"[50]   \n",
      "25                                 \"Dark Horse\"[51]   \n",
      "26                                 \"Let Her Go\"[52]   \n",
      "27                                      \"Faded\"[53]   \n",
      "28                             \"Girls Like You\"[54]   \n",
      "29                                    \"Lean On\"[55]   \n",
      "\n",
      "                                                 Name Artist  \\\n",
      "0         Pinkfong Baby Shark - Kids' Songs & Stories  15.12   \n",
      "1                                          Luis Fonsi   8.55   \n",
      "2   LooLoo Kids - Nursery Rhymes and Children's Songs   6.95   \n",
      "3                          Cocomelon - Nursery Rhymes   6.85   \n",
      "4                          Cocomelon - Nursery Rhymes   6.56   \n",
      "5                                         Wiz Khalifa   6.40   \n",
      "6                                          Ed Sheeran   6.33   \n",
      "7               ChuChu TV Nursery Rhymes & Kids Songs   6.00   \n",
      "8                                         Mark Ronson   5.33   \n",
      "9                                                 Psy   5.30   \n",
      "10                                        Miroshka TV   5.15   \n",
      "11                                         Crazy Frog   4.78   \n",
      "12                                      Ultra Records   4.74   \n",
      "13                                         Get Movies   4.60   \n",
      "14                         Cocomelon - Nursery Rhymes   4.17   \n",
      "15                                       Jingle Toons   4.13   \n",
      "16                                           Maroon 5   4.09   \n",
      "17                                        OneRepublic   4.05   \n",
      "18                                         Katy Perry   4.03   \n",
      "19                                            Shakira   4.01   \n",
      "20                              T-Series Bhakti Sagar   3.97   \n",
      "21      Kiddiestv Hindi - Nursery Rhymes & Kids Songs   3.89   \n",
      "22                                      Justin Bieber   3.84   \n",
      "23                                         Ed Sheeran   3.79   \n",
      "24                                         Ed Sheeran   3.78   \n",
      "25                                         Katy Perry   3.77   \n",
      "26                                          Passenger   3.70   \n",
      "27                                        Alan Walker   3.67   \n",
      "28                                           Maroon 5   3.65   \n",
      "29                               Major Lazer Official   3.65   \n",
      "\n",
      "          Upload Date Views  \n",
      "0       June 17, 2016   [A]  \n",
      "1    January 12, 2017   [B]  \n",
      "2     October 8, 2016        \n",
      "3         May 2, 2018        \n",
      "4        May 24, 2018        \n",
      "5       April 6, 2015   [C]  \n",
      "6    January 30, 2017   [D]  \n",
      "7       March 6, 2014        \n",
      "8   November 19, 2014        \n",
      "9       July 15, 2012   [E]  \n",
      "10  February 27, 2018        \n",
      "11      June 16, 2009        \n",
      "12      April 5, 2018        \n",
      "13   January 31, 2012        \n",
      "14      June 25, 2018        \n",
      "15      June 14, 2018        \n",
      "16   January 14, 2015        \n",
      "17       May 31, 2013        \n",
      "18  September 5, 2013        \n",
      "19       June 4, 2010        \n",
      "20       May 10, 2011        \n",
      "21   January 26, 2018        \n",
      "22   October 22, 2015        \n",
      "23    October 7, 2014        \n",
      "24   November 9, 2017        \n",
      "25  February 20, 2014        \n",
      "26      July 25, 2012        \n",
      "27   December 3, 2015        \n",
      "28       May 31, 2018        \n",
      "29     March 22, 2015        \n"
     ]
    }
   ],
   "source": [
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# URL of the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "# Exception handling for the WebDriver\n",
    "try:\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "    # Finding the table that contains the data\n",
    "    table = driver.find_element(By.XPATH, '//table[contains(@class, \"wikitable\")]')\n",
    "    \n",
    "    # Lists to hold the scraped data\n",
    "    ranks = []\n",
    "    names = []\n",
    "    artists = []\n",
    "    upload_dates = []\n",
    "    views = []\n",
    "\n",
    "    # Iterate through the table rows\n",
    "    rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "    for row in rows[1:]:  # Skip the header row\n",
    "        try:\n",
    "            cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "            ranks.append(cells[0].text)\n",
    "            names.append(cells[1].text)\n",
    "            artists.append(cells[2].text)\n",
    "            upload_dates.append(cells[3].text)\n",
    "            views.append(cells[4].text)\n",
    "        except IndexError:\n",
    "            print(\"Row format may be unexpected; skipping row.\")\n",
    "            continue\n",
    "        except NoSuchElementException as e:\n",
    "            print(f\"No such element: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Create a DataFrame\n",
    "    video_data = pd.DataFrame({\n",
    "        'Rank': ranks,\n",
    "        'Name': names,\n",
    "        'Artist': artists,\n",
    "        'Upload Date': upload_dates,\n",
    "        'Views': views\n",
    "    })\n",
    "\n",
    "    print(video_data)\n",
    "\n",
    "except WebDriverException as e:\n",
    "    print(f\"WebDriver error: {e}\")\n",
    "finally:\n",
    "    driver.quit()  # Ensure the driver is closed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc44f8f",
   "metadata": {},
   "source": [
    "2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Series\n",
    "B) Place\n",
    "C) Date\n",
    "D) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37970fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//a[contains(@href, \"/international-fixtures\")]\"}\n",
      "  (Session info: chrome=129.0.6668.72); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x01197143+25587]\n",
      "\t(No symbol) [0x0112A2E4]\n",
      "\t(No symbol) [0x01022113]\n",
      "\t(No symbol) [0x01066F62]\n",
      "\t(No symbol) [0x010671AB]\n",
      "\t(No symbol) [0x010A7852]\n",
      "\t(No symbol) [0x0108ABE4]\n",
      "\t(No symbol) [0x010A5370]\n",
      "\t(No symbol) [0x0108A936]\n",
      "\t(No symbol) [0x0105BA73]\n",
      "\t(No symbol) [0x0105C4CD]\n",
      "\tGetHandleVerifier [0x01474C63+3030803]\n",
      "\tGetHandleVerifier [0x014C6B99+3366473]\n",
      "\tGetHandleVerifier [0x012295F2+624802]\n",
      "\tGetHandleVerifier [0x01230E6C+655644]\n",
      "\t(No symbol) [0x01132C9D]\n",
      "\t(No symbol) [0x0112FD68]\n",
      "\t(No symbol) [0x0112FF05]\n",
      "\t(No symbol) [0x01122336]\n",
      "\tBaseThreadInitThunk [0x75DA7BA9+25]\n",
      "\tRtlInitializeExceptionChain [0x772CC0CB+107]\n",
      "\tRtlClearBits [0x772CC04F+191]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# URL of the BCCI home page\n",
    "url = \"https://www.bcci.tv/\"\n",
    "\n",
    "# Navigate to the website\n",
    "try:\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "    # Navigate to the International Fixtures section\n",
    "    international_fixtures_button = driver.find_element(By.XPATH, '//a[contains(@href, \"/international-fixtures\")]')\n",
    "    international_fixtures_button.click()\n",
    "    time.sleep(5)  # Wait for the new page to load\n",
    "\n",
    "    # Find the fixtures table\n",
    "    fixtures = driver.find_elements(By.XPATH, '//div[contains(@class, \"fixture-card\")]')\n",
    "\n",
    "    # Lists to hold the scraped data\n",
    "    series = []\n",
    "    places = []\n",
    "    dates = []\n",
    "    times = []\n",
    "\n",
    "    # Iterate through the fixtures and extract data\n",
    "    for fixture in fixtures:\n",
    "        try:\n",
    "            series.append(fixture.find_element(By.XPATH, './/h3').text)  # Series name\n",
    "            place = fixture.find_element(By.XPATH, './/div[contains(@class, \"fixture-card__location\")]').text\n",
    "            places.append(place)\n",
    "            date = fixture.find_element(By.XPATH, './/div[contains(@class, \"fixture-card__date\")]').text\n",
    "            dates.append(date)\n",
    "            time_slot = fixture.find_element(By.XPATH, './/div[contains(@class, \"fixture-card__time\")]').text\n",
    "            times.append(time_slot)\n",
    "        except NoSuchElementException as e:\n",
    "            print(f\"Element not found: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Create a DataFrame\n",
    "    fixture_data = pd.DataFrame({\n",
    "        'Series': series,\n",
    "        'Place': places,\n",
    "        'Date': dates,\n",
    "        'Time': times\n",
    "    })\n",
    "\n",
    "    print(fixture_data)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    driver.quit()  # Ensure the driver is closed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20c9a89",
   "metadata": {},
   "source": [
    "3. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details: A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee9b6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: no such element: Unable to locate element: {\"method\":\"link text\",\"selector\":\"Economy\"}\n",
      "  (Session info: chrome=129.0.6668.72); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x01197143+25587]\n",
      "\t(No symbol) [0x0112A2E4]\n",
      "\t(No symbol) [0x01022113]\n",
      "\t(No symbol) [0x01066F62]\n",
      "\t(No symbol) [0x010671AB]\n",
      "\t(No symbol) [0x010A7852]\n",
      "\t(No symbol) [0x0108ABE4]\n",
      "\t(No symbol) [0x010A5370]\n",
      "\t(No symbol) [0x0108A936]\n",
      "\t(No symbol) [0x0105BA73]\n",
      "\t(No symbol) [0x0105C4CD]\n",
      "\tGetHandleVerifier [0x01474C63+3030803]\n",
      "\tGetHandleVerifier [0x014C6B99+3366473]\n",
      "\tGetHandleVerifier [0x012295F2+624802]\n",
      "\tGetHandleVerifier [0x01230E6C+655644]\n",
      "\t(No symbol) [0x01132C9D]\n",
      "\t(No symbol) [0x0112FD68]\n",
      "\t(No symbol) [0x0112FF05]\n",
      "\t(No symbol) [0x01122336]\n",
      "\tBaseThreadInitThunk [0x75DA7BA9+25]\n",
      "\tRtlInitializeExceptionChain [0x772CC0CB+107]\n",
      "\tRtlClearBits [0x772CC04F+191]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# URL of the Statistics Times home page\n",
    "url = \"http://statisticstimes.com/\"\n",
    "\n",
    "try:\n",
    "    # Navigate to the website\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "    # Navigate to the Economy page\n",
    "    economy_link = driver.find_element(By.LINK_TEXT, 'Economy')\n",
    "    economy_link.click()\n",
    "    time.sleep(3)  # Wait for the new page to load\n",
    "\n",
    "    # Navigate to the State-wise GDP page\n",
    "    gdp_link = driver.find_element(By.LINK_TEXT, 'State-wise GDP of India')\n",
    "    gdp_link.click()\n",
    "    time.sleep(3)  # Wait for the new page to load\n",
    "\n",
    "    # Find the table that contains the data\n",
    "    table = driver.find_element(By.XPATH, '//table[@id=\"table_id\"]')\n",
    "    \n",
    "    # Lists to hold the scraped data\n",
    "    ranks = []\n",
    "    states = []\n",
    "    gsdps_18_19 = []\n",
    "    gsdps_19_20 = []\n",
    "    shares_18_19 = []\n",
    "    gdp_billion = []\n",
    "\n",
    "    # Iterate through the table rows\n",
    "    rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "    for row in rows[1:]:  # Skip the header row\n",
    "        cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "        if len(cells) > 0:  # Ensure there are cells in the row\n",
    "            ranks.append(cells[0].text)\n",
    "            states.append(cells[1].text)\n",
    "            gsdps_18_19.append(cells[2].text)\n",
    "            gsdps_19_20.append(cells[3].text)\n",
    "            shares_18_19.append(cells[4].text)\n",
    "            gdp_billion.append(cells[5].text)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    gdp_data = pd.DataFrame({\n",
    "        'Rank': ranks,\n",
    "        'State': states,\n",
    "        'GSDP(18-19)': gsdps_18_19,\n",
    "        'GSDP(19-20)': gsdps_19_20,\n",
    "        'Share(18-19)': shares_18_19,\n",
    "        'GDP($ billion)': gdp_billion\n",
    "    })\n",
    "\n",
    "    print(gdp_data)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    driver.quit()  # Ensure the driver is closed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42735030",
   "metadata": {},
   "source": [
    "4. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d881c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//summary[contains(text(), \"Explore\")]\"}\n",
      "  (Session info: chrome=129.0.6668.72); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x01197143+25587]\n",
      "\t(No symbol) [0x0112A2E4]\n",
      "\t(No symbol) [0x01022113]\n",
      "\t(No symbol) [0x01066F62]\n",
      "\t(No symbol) [0x010671AB]\n",
      "\t(No symbol) [0x010A7852]\n",
      "\t(No symbol) [0x0108ABE4]\n",
      "\t(No symbol) [0x010A5370]\n",
      "\t(No symbol) [0x0108A936]\n",
      "\t(No symbol) [0x0105BA73]\n",
      "\t(No symbol) [0x0105C4CD]\n",
      "\tGetHandleVerifier [0x01474C63+3030803]\n",
      "\tGetHandleVerifier [0x014C6B99+3366473]\n",
      "\tGetHandleVerifier [0x012295F2+624802]\n",
      "\tGetHandleVerifier [0x01230E6C+655644]\n",
      "\t(No symbol) [0x01132C9D]\n",
      "\t(No symbol) [0x0112FD68]\n",
      "\t(No symbol) [0x0112FF05]\n",
      "\t(No symbol) [0x01122336]\n",
      "\tBaseThreadInitThunk [0x75DA7BA9+25]\n",
      "\tRtlInitializeExceptionChain [0x772CC0CB+107]\n",
      "\tRtlClearBits [0x772CC04F+191]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# URL of the GitHub home page\n",
    "url = \"https://github.com/\"\n",
    "\n",
    "try:\n",
    "    # Navigate to the website\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "    # Navigate to the Trending repositories\n",
    "    explore_button = driver.find_element(By.XPATH, '//summary[contains(text(), \"Explore\")]')\n",
    "    explore_button.click()\n",
    "    time.sleep(1)  # Wait for the dropdown to open\n",
    "\n",
    "    trending_button = driver.find_element(By.LINK_TEXT, 'Trending')\n",
    "    trending_button.click()\n",
    "    time.sleep(3)  # Wait for the trending page to load\n",
    "\n",
    "    # Find all trending repositories\n",
    "    repositories = driver.find_elements(By.XPATH, '//article[@class=\"Box-row\"]')\n",
    "\n",
    "    # Lists to hold the scraped data\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    contributors = []\n",
    "    languages = []\n",
    "\n",
    "    # Iterate through the repositories and extract data\n",
    "    for repo in repositories:\n",
    "        try:\n",
    "            title = repo.find_element(By.XPATH, './/h1[@class=\"h3 lh-condensed\"]').text.strip()\n",
    "            description = repo.find_element(By.XPATH, './/p[@class=\"col-9 color-fg-muted my-1 pr-4\"]').text.strip()\n",
    "            contributor_count = repo.find_element(By.XPATH, './/a[@class=\"Link--secondary\"]').text.strip()\n",
    "            language = repo.find_element(By.XPATH, './/span[@itemprop=\"programmingLanguage\"]').text.strip()\n",
    "\n",
    "            titles.append(title)\n",
    "            descriptions.append(description)\n",
    "            contributors.append(contributor_count)\n",
    "            languages.append(language)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing a repository: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Create a DataFrame\n",
    "    trending_data = pd.DataFrame({\n",
    "        'Repository Title': titles,\n",
    "        'Description': descriptions,\n",
    "        'Contributors Count': contributors,\n",
    "        'Language Used': languages\n",
    "    })\n",
    "\n",
    "    print(trending_data)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    driver.quit()  # Ensure the driver is closed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d4ca1",
   "metadata": {},
   "source": [
    "5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the\n",
    "following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    " Note: - From the home page you have to click on the charts option then hot 100-page link through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc695ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: element click intercepted: Element is not clickable at point (969, 10990)\n",
      "  (Session info: chrome=129.0.6668.72)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x01197143+25587]\n",
      "\t(No symbol) [0x0112A2E4]\n",
      "\t(No symbol) [0x01022113]\n",
      "\t(No symbol) [0x0106CEB7]\n",
      "\t(No symbol) [0x0106B2B9]\n",
      "\t(No symbol) [0x010691AB]\n",
      "\t(No symbol) [0x01068798]\n",
      "\t(No symbol) [0x0105D58D]\n",
      "\t(No symbol) [0x0108AB9C]\n",
      "\t(No symbol) [0x0105D044]\n",
      "\t(No symbol) [0x0108AE34]\n",
      "\t(No symbol) [0x010A5370]\n",
      "\t(No symbol) [0x0108A936]\n",
      "\t(No symbol) [0x0105BA73]\n",
      "\t(No symbol) [0x0105C4CD]\n",
      "\tGetHandleVerifier [0x01474C63+3030803]\n",
      "\tGetHandleVerifier [0x014C6B99+3366473]\n",
      "\tGetHandleVerifier [0x012295F2+624802]\n",
      "\tGetHandleVerifier [0x01230E6C+655644]\n",
      "\t(No symbol) [0x01132C9D]\n",
      "\t(No symbol) [0x0112FD68]\n",
      "\t(No symbol) [0x0112FF05]\n",
      "\t(No symbol) [0x01122336]\n",
      "\tBaseThreadInitThunk [0x75DA7BA9+25]\n",
      "\tRtlInitializeExceptionChain [0x772CC0CB+107]\n",
      "\tRtlClearBits [0x772CC04F+191]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# URL of the Billboard home page\n",
    "url = \"https://www.billboard.com/\"\n",
    "\n",
    "try:\n",
    "    # Navigate to the website\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "    # Navigate to the Charts page\n",
    "    charts_button = driver.find_element(By.LINK_TEXT, 'Charts')\n",
    "    charts_button.click()\n",
    "    time.sleep(3)  # Wait for the charts page to load\n",
    "\n",
    "    # Click on the Hot 100 link\n",
    "    hot_100_link = driver.find_element(By.LINK_TEXT, 'Hot 100')\n",
    "    hot_100_link.click()\n",
    "    time.sleep(3)  # Wait for the Hot 100 page to load\n",
    "\n",
    "    # Find all top songs\n",
    "    songs = driver.find_elements(By.XPATH, '//li[@class=\"o-chart-results-list-row-container\"]')\n",
    "\n",
    "    # Lists to hold the scraped data\n",
    "    song_names = []\n",
    "    artist_names = []\n",
    "    last_week_ranks = []\n",
    "    peak_ranks = []\n",
    "    weeks_on_board = []\n",
    "\n",
    "    # Iterate through the top songs and extract data\n",
    "    for song in songs[:100]:  # Limit to top 100 songs\n",
    "        try:\n",
    "            song_name = song.find_element(By.XPATH, './/h3[@id=\"title-of-a-story\"]').text.strip()\n",
    "            artist_name = song.find_element(By.XPATH, './/span[@class=\"a-no-trucate\"]').text.strip()\n",
    "            last_week_rank = song.find_element(By.XPATH, './/span[@class=\"c-label a-font-primary a-font-size-base a-font-weight-bold\"]').text.strip()\n",
    "            peak_rank = song.find_elements(By.XPATH, './/span[@class=\"c-label a-font-primary a-font-size-base a-font-weight-bold\"]')[1].text.strip()\n",
    "            weeks_on_board = song.find_element(By.XPATH, './/span[@class=\"c-label a-font-primary a-font-size-base a-font-weight-bold\"]').text.strip()\n",
    "\n",
    "            song_names.append(song_name)\n",
    "            artist_names.append(artist_name)\n",
    "            last_week_ranks.append(last_week_rank)\n",
    "            peak_ranks.append(peak_rank)\n",
    "            weeks_on_board.append(weeks_on_board)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing a song: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Create a DataFrame\n",
    "    top_songs_data = pd.DataFrame({\n",
    "        'Song Name': song_names,\n",
    "        'Artist Name': artist_names,\n",
    "        'Last Week Rank': last_week_ranks,\n",
    "        'Peak Rank': peak_ranks,\n",
    "        'Weeks on Board': weeks_on_board\n",
    "    })\n",
    "\n",
    "    print(top_songs_data)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    driver.quit()  # Ensure the driver is closed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e24308",
   "metadata": {},
   "source": [
    "6. Scrape the details of Highest selling novels.\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n",
    " Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c257c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = 'https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare'\n",
    "\n",
    "# Send a GET request to fetch the page content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Define lists to hold book details\n",
    "book_names = []\n",
    "author_names = []\n",
    "volumes_sold = []\n",
    "publishers = []\n",
    "genres = []\n",
    "\n",
    "# Scrape the table data\n",
    "table = soup.find('table')  # Find the first table on the page\n",
    "if table:\n",
    "    rows = table.find_all('tr')  # Get all the rows in the table\n",
    "\n",
    "    # Loop through rows and extract data\n",
    "    for row in rows[1:]:  # Skip the header row\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 5:  # Check if there are enough columns\n",
    "            book_names.append(columns[0].text.strip())\n",
    "            author_names.append(columns[1].text.strip())\n",
    "            volumes_sold.append(columns[2].text.strip())\n",
    "            publishers.append(columns[3].text.strip())\n",
    "            genres.append(columns[4].text.strip())\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "df = pd.DataFrame({\n",
    "    'Book Name': book_names,\n",
    "    'Author Name': author_names,\n",
    "    'Volumes Sold': volumes_sold,\n",
    "    'Publisher': publishers,\n",
    "    'Genre': genres\n",
    "})\n",
    "\n",
    "# Display the scraped data\n",
    "df.head()\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv('best_selling_books.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18655e28",
   "metadata": {},
   "source": [
    "7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/ You have\n",
    "to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0111142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the IMDb page\n",
    "url = 'https://www.imdb.com/list/ls095964455/'\n",
    "\n",
    "# Send a GET request to fetch the page content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Lists to hold the scraped details\n",
    "names = []\n",
    "year_spans = []\n",
    "genres = []\n",
    "run_times = []\n",
    "ratings = []\n",
    "votes = []\n",
    "\n",
    "# Find all the TV series items\n",
    "series_list = soup.find_all('div', class_='lister-item mode-detail')\n",
    "\n",
    "for series in series_list:\n",
    "    # Scrape the name of the TV series\n",
    "    name = series.h3.a.text.strip()\n",
    "    names.append(name)\n",
    "    \n",
    "    # Scrape the year span\n",
    "    year_span = series.find('span', class_='lister-item-year').text.strip()\n",
    "    year_spans.append(year_span)\n",
    "    \n",
    "    # Scrape the genre\n",
    "    genre = series.find('span', class_='genre').text.strip()\n",
    "    genres.append(genre)\n",
    "    \n",
    "    # Scrape the runtime\n",
    "    run_time = series.find('span', class_='runtime').text.strip() if series.find('span', class_='runtime') else 'N/A'\n",
    "    run_times.append(run_time)\n",
    "    \n",
    "    # Scrape the rating\n",
    "    rating = series.find('span', class_='ipl-rating-star__rating').text.strip() if series.find('span', class_='ipl-rating-star__rating') else 'N/A'\n",
    "    ratings.append(rating)\n",
    "    \n",
    "    # Scrape the votes\n",
    "    vote = series.find('span', attrs={'name': 'nv'}).text.strip() if series.find('span', attrs={'name': 'nv'}) else 'N/A'\n",
    "    votes.append(vote)\n",
    "\n",
    "# Create a DataFrame to store the scraped data\n",
    "df = pd.DataFrame({\n",
    "    'Name': names,\n",
    "    'Year Span': year_spans,\n",
    "    'Genre': genres,\n",
    "    'Run Time': run_times,\n",
    "    'Ratings': ratings,\n",
    "    'Votes': votes\n",
    "})\n",
    "\n",
    "# Display the scraped data\n",
    "df.head()\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv('most_watched_tv_series.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5164da6e",
   "metadata": {},
   "source": [
    "8. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/ You\n",
    "have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute G) Year\n",
    " Note: - from the home page you have to go to the Show All Dataset page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc7f75cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " /\n",
      "Datasets /datasets\n",
      "Contribute Dataset /contribute/donation\n",
      "Donate New /contribute/donation\n",
      "Link External /contribute/linking\n",
      "About Us /about\n",
      "Who We Are /about\n",
      "Citation Metadata /citation\n",
      "Contact Information /contact\n",
      "Login /auth/login\n",
      "View Datasets /datasets\n",
      "Contribute a Dataset /contribute/donation/\n",
      " /dataset/53/iris\n",
      "Iris /dataset/53/iris\n",
      " /dataset/45/heart+disease\n",
      "Heart Disease /dataset/45/heart+disease\n",
      " /dataset/186/wine+quality\n",
      "Wine Quality /dataset/186/wine+quality\n",
      " /dataset/2/adult\n",
      "Adult /dataset/2/adult\n",
      " /dataset/17/breast+cancer+wisconsin+diagnostic\n",
      "Breast Cancer Wisconsin (Diagnostic) /dataset/17/breast+cancer+wisconsin+diagnostic\n",
      " /dataset/222/bank+marketing\n",
      "Bank Marketing /dataset/222/bank+marketing\n",
      "See More Popular Datasets /datasets\n",
      " /dataset/1031/dataset+for+assessing+mathematics+learning+in+higher+education\n",
      "Dataset for Assessing Mathematics Learning in Higher Education /dataset/1031/dataset+for+assessing+mathematics+learning+in+higher+education\n",
      " /dataset/1025/turkish+crowdfunding+startups\n",
      "Turkish Crowdfunding Startups /dataset/1025/turkish+crowdfunding+startups\n",
      " /dataset/1013/synthetic+circle+data+set\n",
      "Synthetic Circle Data Set /dataset/1013/synthetic+circle+data+set\n",
      " /dataset/994/micro+gas+turbine+electrical+energy+prediction\n",
      "Micro Gas Turbine Electrical Energy Prediction /dataset/994/micro+gas+turbine+electrical+energy+prediction\n",
      " /dataset/990/printed+circuit+board+processed+image\n",
      "Printed Circuit Board Processed Image /dataset/990/printed+circuit+board+processed+image\n",
      " /dataset/967/phiusiil+phishing+url+dataset\n",
      "PhiUSIIL Phishing URL (Website) /dataset/967/phiusiil+phishing+url+dataset\n",
      "See More New Datasets /datasets?orderBy=DateDonated&sort=desc\n",
      "Read Policy /privacy\n",
      " /\n",
      "About Us /about\n",
      "CML https://cml.ics.uci.edu\n",
      "National Science Foundation https://www.nsf.gov\n",
      "Home /\n",
      "View Datasets /datasets\n",
      "Donate a Dataset /contribute/donation\n",
      "Contact /contact\n",
      "Privacy Notice /privacy\n",
      "Feature Request or Bug Report https://github.com/uci-ml-repo/ucimlrepo-feedback/issues/new/choose\n",
      "Browse Datasets /datasets\n",
      "Donate a Dataset /contribute/donation\n",
      "Link an external Dataset /contribute/linking\n",
      "Who We Are /about\n",
      "Citation Metadata /citation\n",
      "Contact Information /contact\n",
      "Login /auth/login\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Base URL of UCI Machine Learning Repository\n",
    "base_url = 'https://archive.ics.uci.edu'\n",
    "\n",
    "# URL of the main page\n",
    "url = base_url + '/ml/index.php'\n",
    "\n",
    "# Send a GET request to fetch the homepage content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Print all links to identify the correct one for \"View All Datasets\"\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.text.strip(), link.get('href'))\n",
    "\n",
    "# Manually find the correct link based on printed output, for now I'm assuming it's '/ml/datasets.php'\n",
    "datasets_link = '/ml/datasets.php'\n",
    "datasets_url = base_url + datasets_link\n",
    "\n",
    "# Send a GET request to fetch the datasets page content\n",
    "datasets_response = requests.get(datasets_url)\n",
    "datasets_soup = BeautifulSoup(datasets_response.text, 'html.parser')\n",
    "\n",
    "# Lists to store scraped data\n",
    "dataset_names = []\n",
    "data_types = []\n",
    "tasks = []\n",
    "attribute_types = []\n",
    "no_of_instances = []\n",
    "no_of_attributes = []\n",
    "years = []\n",
    "\n",
    "# Find all dataset containers\n",
    "dataset_rows = datasets_soup.find_all('tr')[1:]  # Skipping the header row\n",
    "\n",
    "# Loop through each row and extract the required details\n",
    "for row in dataset_rows:\n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    dataset_names.append(columns[0].text.strip())\n",
    "    data_types.append(columns[1].text.strip())\n",
    "    tasks.append(columns[2].text.strip())\n",
    "    attribute_types.append(columns[3].text.strip())\n",
    "    no_of_instances.append(columns[4].text.strip())\n",
    "    no_of_attributes.append(columns[5].text.strip())\n",
    "    years.append(columns[6].text.strip())\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "df = pd.DataFrame({\n",
    "    'Dataset Name': dataset_names,\n",
    "    'Data Type': data_types,\n",
    "    'Task': tasks,\n",
    "    'Attribute Type': attribute_types,\n",
    "    'No of Instances': no_of_instances,\n",
    "    'No of Attributes': no_of_attributes,\n",
    "    'Year': years\n",
    "})\n",
    "\n",
    "# Display the scraped data\n",
    "df.head()\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv('uci_datasets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa4855c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
